%%%%%%%%%%%%%%%%%%%%%definitions%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{../header.tex}
\input{../newcommands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%DOCUMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%\preprint{}

\title{The parallel derivative on structured grids}
\author{M.~Wiesenberger}
%\email{Matthias.Wiesenberger@uibk.ac.at}
%\ead{Matthias.Wiesenberger@uibk.ac.at}
%\affiliation{Institute for Ion Physics and Applied Physics, Association EURATOM-\"OAW,  University of
%   Innsbruck, A-6020 Innsbruck, Austria}
%\author{M.~Held}
%\affiliation{Institute for Ion Physics and Applied Physics, Association EURATOM-\"OAW,  University of
   %Innsbruck, A-6020 Innsbruck, Austria}
%\address{Institute for Ion Physics and Applied Physics, Association EURATOM-\"OAW,  Universit\"at 
%   Innsbruck, A-6020 Innsbruck, Austria}
 
\maketitle
\section{Semi-Lagrangian schemes} \label{sec:parallel}
In this section we show how to numerically treat parallel derivatives in a non field
aligned coordinate system.
We introduce the method in Section~\ref{sec:parallela} %.In Section~\ref{sec:parallelb} we
and discuss some problems arising from the boundaries of the computational domain.
Finally, we propose a field line mapping for variable initialization in Section~\ref{sec:parallelc}.

\subsection{Discretization of parallel derivatives} \label{sec:parallela}
One idea to discretize $\bhat\cdot\nabla$ is to simply use 
the dG discretization developed in the last section directly, i.e.~discretize $b^R\partial_R+b^Z\partial_Z+b^\varphi\partial_\varphi$, where $(R,Z,\varphi)$ are cylindrical coordinates. 
However, a very restrictive CFL-condition due to the high resolution
in the $R$-$Z$ planes and the fast parallel motion makes 
this approach impractical. We therefore decided to use a 
Lagrangian approach for the parallel derivatives.

We begin with the formulation of a field-aligned discretization. If $s$ denotes the 
field line following coordinate, then the one-dimensional discrete derivative along the field line reads
\begin{align}
    \frac{\d f }{\d s} \rightarrow \frac{f_{k+1}-f_{k-1}}{s_{k+1} - s_{k-1}}.
    \label{eq:proposition}
\end{align}

From differential geometry we 
know that to every smooth vector field $\bhat$ there is a unique curve of which the
tangent in a point is the value of $\bhat$ at that point. It is given by
the solution of the differential equation
\begin{align}
    \frac{\d z^i}{\d s} = \hat b^i(\vec z)
    \label{eq:integralcurve}
\end{align}
where $z^i$ is one of $(R, Z, \varphi)$ and $\hat b^i$ are the contravariant components
of $\bhat$ in cylindrical coordinates. 
Moreover, by definition we have
\begin{align}
    \frac{\d f(\vec z(s))}{\d s} = \bhat\cdot \nabla f|_{\vec z(s)}
    \label{}
\end{align}
along a field line parameterized by distance $s$, 
i.e. instead of $\nabla_\parallel f$ we can choose to discretize $\frac{\d f}{\d s}$.

Let us divide the $\varphi$ direction into $N_\varphi$ equidistant planes of distance
$\Delta \varphi$. Unfortunately, from Eq.~\eqref{eq:integralcurve} we cannot easily determine the 
distance $\Delta s$ for given $\Delta \varphi$. It is better to integrate
\begin{align}
    \frac{\d z^i}{\d t}=\frac{b^i}{b^\varphi} = \frac{B^i}{B^\varphi}
    \label{}
\end{align}
since in this case $\d\varphi/\d t = 1 \Rightarrow t=\varphi$. We get
\begin{subequations}
\begin{align}
    \frac{\d R}{\d\varphi}&= \frac{B^R}{B^\varphi},\\ %\frac{R}{I}\frac{\partial\psi}{\partial Z},\\
    \frac{\d Z}{\d\varphi}&=\frac{B^Z}{B^\varphi},%-\frac{R}{I}\frac{\partial\psi}{\partial R}.
\end{align}
\text{ together with the equation  }
\begin{align}
    \frac{\d s}{\d\varphi} &= \frac{1}{|\hat b^\varphi|} = \frac{B}{|B^\varphi|}% = \frac{R_0R^2B}{I}=R_0R\sqrt{1+\frac{|\nabla\psi|^2}{I^2}}
    \label{eq:fieldlinec}
\end{align}
\label{eq:fieldline}
\end{subequations}
for the length of the field line $s$. 
Eqs.~\eqref{eq:fieldline} are integrated from $\varphi=0$ to $\varphi=\pm \Delta \varphi$. 
We characterize the flow generated by $\bhat/b^\varphi$ by
\begin{align}
    T_{\Delta \varphi}^{\pm 1}\vec z := T_{\Delta \varphi}^{\pm 1}[R, Z, \varphi]:= ( R(\pm \Delta\varphi), Z( \pm \Delta\varphi), \varphi\pm\Delta \varphi),
    \label{}
\end{align}
where $(R(\varphi), Z(\varphi), s(\varphi))$ is the solution to Eqs.~\eqref{eq:fieldline} 
with initial condition 
\begin{align}
    (R(0), Z(0), s(0)) = (R, Z, 0).
    \label{}
\end{align} 
Obviously we have $T^{-1}_{\Delta\varphi}\circ T^{+1}_{\Delta\varphi} = \Eins$, but $T^{\pm}_{\Delta\varphi}$ is not unitary since $\bhat/b^\varphi$ is 
not divergence free. 

The proposed centered discretization~\eqref{eq:proposition} for the parallel derivative then reads
\begin{align}
    \nabla_\parallel f \equiv \frac{df}{ds} = \frac{df}{d\varphi}\frac{d\varphi}{ds} 
    \rightarrow \frac{f\left(T_{\Delta\varphi}^+\vec z\right)-f\left(T_{\Delta\varphi}^-\vec z\right)}{s(+\Delta\varphi) - s(-\Delta\varphi)},
    \label{eq:paralleldis}
\end{align}
which is slightly different from Reference~\cite{Hariri2014}, where
the relation~\eqref{eq:fieldlinec} was used to replace $\d \varphi/\d s$. 
Previous derivations now needed to construct an interpolation scheme in order
to evaluate functions on the transformed coordinates. 
Since the $R$ and $Z$ coordinates are still discretized in the dG framework we note that in our work
the interpolation of $f$ on the transformed points $T_{\Delta\varphi}^{\pm 1}\vec z$
is naturally given by Eq.~\eqref{eq:dgexpansion} (the extension to two dimensions is immediate). 
Let us for a moment omit the $Z$ coordinate for ease of notation. If $(R_{nj}, \varphi_k)$ are the grid points, 
we call $(R^+_{nj}, \varphi_{k+1}) := T^+_{\Delta\varphi}[R_{nj}, \varphi_k]$ and $(R_{nj}^-, \varphi_{k-1}) := T^-_{\Delta\varphi}[R_{nj}, \varphi_k]$ the transformed coordinates along
the field lines. We then have
\begin{subequations}
\begin{align}
    f(T^+_{\Delta\varphi}\vec z) = f( R^+_{nj}, \varphi_{k+1}) = \bar f_{k+1}^{ml}p_{ml}(R^+_{nj}) =: (I^+)_{nj}^{ml}f_{(k+1)ml} , \\
    f(T^-_{\Delta\varphi}\vec z) = f( R^-_{nj}, \varphi_{k-1}) = \bar f_{k-1}^{ml}p_{ml}(R^-_{nj}) =: (I^-)_{nj}^{ml}f_{(k-1)ml} , 
\end{align}
\label{eq:interpolation}
\end{subequations}
where the backward transformations of $\bar{ \vec f}$ are hidden in $I$.
Thus the interpolation of all the necessary points can simply be written as a matrix-vector product, where the interpolation matrices $I^+$  and $I^-$ are independent of time since
the field lines are constant in time. The order of this interpolation is given by $P$, the number of polynomial coefficients.
A consistency check is the relation $I^+\circ I^- = \Eins$. 

The discretization~\eqref{eq:paralleldis} can now be written as a matrix vector product
\begin{align}
    \nabla_\parallel f \rightarrow S \circ \left[ \Eins^+\otimes I^+ - \Eins^- \otimes I^-  \right] \vec f, 
    \label{}
\end{align}
where $S$ is the diagonal matrix that contains the entries $1/(s(+\Delta\varphi) - s(-\Delta\varphi))$.
This discretization is not skew-symmetric since the
field lines are not volume-preserving, or~$(I^+)^\mathrm{T} \neq I^-$.
In fact, the adjoint of the parallel derivative is
\begin{align}
    \nabla_\parallel^\dagger = - \nabla\cdot(\bhat\ . ) \neq -\nabla_\parallel.
    \label{}
\end{align}
Note that with this relation we can define the parallel 
diffusion operator as
\begin{align}
    \Delta_\parallel := -\nabla_\parallel^\dagger \nabla_\parallel = (\nabla\cdot \bhat) \nabla_\parallel + \nabla_\parallel^2 , 
    \label{}
\end{align}
which is indeed the parallel part of the full Laplacian $\Delta = \nabla\cdot( \bhat \nabla_\parallel + \nabla_\perp)$.
The second order derivative $\nabla_\parallel^2$ can be 
discretized using 
\begin{align}
    \frac{\d^2 f}{\d s^2} \rightarrow  
    \frac{2f_{k+1} }{(s_{k+1}-s_k)(s_{k+1}-s_{k-1})} -
    \frac{2f_k}{(s_{k+1}-s_k)(s_k - s_{k-1})} \nonumber\\ + 
    \frac{2f_{k-1} }{(s_{k}-s_{k-1})(s_{k+1}-s_{k-1})} 
    \label{}
\end{align}
and repeating the procedure of this section.

%We see that the term in brackets is obviously skew-symmetric if $(I^+)^\mathrm{T} = I^- = (I^+)^{-1}$, i.e.~the interpolation matrix is unitary.
% IT ISNT
%\subsection{Limiter and Cutting procedures} \label{sec:parallelb}
The main problem with the above scheme is the question what
 to do when a field line crosses the simulation boundaries. 
Boundary conditions are formulated in cylindrical coordinates. 
One idea is to simply cut the contribution from field lines
that leave the computational domain. While this works in practice
it is unclear what numerical and physical side-effects this procedure might have. 
Another idea is to check to every point $\vec z$ whether $T_{\Delta\varphi}\vec z$
lies inside our simulation box or not. If not, we have to find where exactly the 
field line intersects the simulation box.  We have to find
$\varphi_b$ such that the result of the integration of Eq.~\eqref{eq:fieldline} from 
$0$ to $\varphi_b$ lies on the boundary. 
The angle $\varphi_b$ can be found by a bisection algorithm knowing that $0<\varphi_b < \Delta\varphi$. 
This kind of procedure is known as a shooting method. 
When all points are found, ghost cells can be constructed in the correct way. 

A poloidal limiter can simply be implemented via a boundary condition in $\varphi$. 
As long as the form of the limiter is a flux-function we do not have to 
integrate a field line in order to determine which points lie in the
limiter-shadow. It is therefore straightforward to implement ghost-cells 
in that case. 

\subsection{Field aligned initialization} \label{sec:parallelc}

An important aspect of our simulations is a judicious initialization of the 
fields. We want structures to be field-aligned in the beginning of the simulation with
a possible modulation along the direction of the field line.
If a Gaussian shape is used, we call $\sigma_\parallel$ the extension in parallel
direction and write
\begin{align}
    f_0(R,Z,\varphi) = F(R,Z,\varphi) \exp\left( - \frac{(\varphi-\varphi_0)^2}{2\sigma_\parallel^2}\right),
    \label{eq:parallelInit}
\end{align}
where $F$ is a function that is invariant under the field line transformations
\begin{subequations}
\begin{align}
    T_{\Delta\varphi}^+ F(\vec z) &= F( T_{\Delta\varphi}^+\vec z) \overset{!}{=} F(\vec z) \text{ (pull-back), } \\
    T_{\Delta\varphi}^- F(\vec z) &= F( T_{\Delta\varphi}^-\vec z) \overset{!}{=} F(\vec z) \text{ (push-forward). } 
\end{align}
\label{}
\end{subequations}
We can use these relations to construct aligned structures
by active transformations of some given field.
Our idea is to initialize a two-dimensional field $F(R,Z, \varphi_k)$ in a given plane $k$ and 
transform this field to all other planes using the recursive relations
\begin{subequations}
\begin{align}
    F( R, Z, \varphi_{k+1}) = T_{\Delta\varphi}^- F( R, Z, \varphi_{k+1}) = F(R^-, Z^-, \varphi_k), \\
    F( R, Z, \varphi_{k-1}) = T_{\Delta\varphi}^+ F( R, Z, \varphi_{k-1}) = F(R^+, Z^+, \varphi_k),
\end{align}
    \label{eq:recursiveInit}
\end{subequations}
which is the statement that $F$ in the next plane equals the push-forward  
and $F$ in the previous plane equals the pull-back of $F$ in the current plane. 
Note here that Eq.~\eqref{eq:interpolation} applies for the required interpolation
procedures. 

\section{The parallel derivative}
The idea of the sandwich method \cite{Held2016} is to bracket the parallel derivative by interpolation and projection matrices:
\begin{align}
    \nabla^c_\parallel &= P\nabla_\parallel^f Q \\
    \nabla^{c\dagger}_\parallel &= P \nabla^{f\dagger}_\parallel Q
    \label{eq:sandwich}
\end{align}
In this way the projection integrals
\begin{align}
    \int\dV \nabla_\parallel f p_i(x)p_j(y) 
    \label{}
\end{align}
are computed more precisely.
The size of the fine grid should therefore be as large as
possible (reasonable? what resolution is needed?).
We first notice that the one interpolation matrix can be absorbed
in the parallel derivative since this also consists of 
interpolation operations. 
\begin{align}
    \nabla^c_\parallel &= P\nabla_\parallel^{fc} \\
    \nabla^{c\dagger}_\parallel &= \nabla^{fc\dagger}_\parallel Q
    \label{eq:sandwich}
\end{align}


In order to understand what the adjoint operators do let us denote $\mathcal T^+_{\Delta\varphi}$ as the push-forwward operator. Then we have
\begin{align}
    \int f(\vec x) \Tp h(\vec x) \sqrt{g(\vec x)}\d^3x \\
    =  \int f(\vec x) h(\Tm \vec x)\sqrt{g(\vec x)}\d^3x \\
    =  \int f(\Tp \vec x') h(\vec x')\sqrt{g(\Tp \vec x')}J^{-1}( \Tp\vec x') \d^3x' \\
    =  \int \frac{1}{\sqrt{g(\vec x')}}\Tm\left[J^{-1}(\vec x')\sqrt{g(\vec x')}f(\vec x')\right] h(\vec x')\sqrt{g(\vec x')}   \d^3x' \\
    \equiv  \int (\Tp)^\dagger\left[f(\vec x)\right] h(\vec x)\sqrt{g(\vec x)}   \d^3x
    \label{}
\end{align}
$J$ is the determinant of the Jacobian $\partial(\vec x')/\partial(\vec x)$.
In the last step we simply replaced the dummy variable $\vec x'$ with $\vec x$ again and identified the relevant terms
as the adjoint operator:
\begin{align}
    (\Tp)^\dagger f(\vec x ) := \frac{1}{\sqrt{g(\vec x)}} \Tm\left[\sqrt{g(\vec x)} J^{-1}(\vec x) f(\vec x) \right]
    \label{}
\end{align}
This means that numerically the adjoint of the push-forward 
operator should be a valid discretization of its inverse. (how can this be exploited as a test?)
Note that $\sqrt{g}J^{-1}(\vec x) = \sqrt{g'(\Tm \vec x)}$.
With this we can write
\begin{align}
    (\Tp)^\dagger f(\vec x ) := \sqrt{\frac{g'(\vec x)}{g(\vec x)}} \Tm\left[f(\vec x) \right]
    \label{}
\end{align}
Also note that while $\Tp [fh] = \Tp f \Tp h$ this might not 
hold on the discrete level. Also the question is how $J$ enters 
on the discrete level. We have to multiply $\sqrt{g}$ artificially when we form the adjoint. 
Theoretically $J$ could be hidden somehow when we integrate the fieldlines, so the information could be contained in the discrete version? (Maybe in the back-projection?)

If the streamlines are divergence free, we have $J=1$.
A numerical test could be ( if we neglect the volume form in the adjoint)
\begin{align}
    (\Tp)^\dagger \left[J(\vec x)\Tp f(\vec x)\right] - f(\vec x) = 0
    \label{}
\end{align}
The numerical computation of $J$ might a bit tricky at the boundaries. 
In a flux-aligned $\zeta, \eta$ it should be feasible but in cylindrical coordinates I don't know how. Maybe we can simply cut the last few cells before the boundary.
Even easier might be
\begin{align}
    \left(\Tp\right)^\dagger J(\vec x ) = 1
    \label{}
\end{align}

If we integrate streamlines of the vector field $\vec B/B^\varphi$, then we have
\begin{align}
    \frac{\d J}{\d \varphi} = J(\vec x ) \nabla\cdot\left( \vec B/B^\varphi\right)
    \label{}
\end{align}
along these streamlines.
Also we have that $\d s = B/B^\varphi \d \varphi $ and the interpolation/projection of $\triangle s$ can probably be neglected. (We want to neglect it because it's memory intensive to store all combinations)
Also this means that when we transpose $\nabla_\parallel$ we get a 
multiplication by $B^\varphi/B$ in the beginning.
In any case this means that we want to have 
\begin{align}
\left(\Tp\right)^\dagger B^\varphi  =  B^\varphi
\end{align}
\section{Boundary conditions}
The question is what to do when a fieldline intersects with the boundary
of the simulation domain before reaching the next plane.
The problem with simply putting a value exactly where the fieldline 
reaches the boundary is first that a true interpolation at that 
position requires a 3d interpolation (we 
are between planes) instead of a 2d interpolation.
Secondly, for Dirichlet boundaries the small 
distance seriously deteriorates the CFL condition.
A simple solution is to set the perpendicular field zero on the 
boundary such that the field becomes purely 
toroidal on the boundary. The fieldlines then have a kink on the 
boundary. On the other hand we can implement boundary conditions consistent with 
the perpendicular ones since the fieldlines never leave the domain. 
We simply interpolate the quantity to derive on the inner side of the
domain boundary (Neumann conditions = "No boundary condition") or 
set the value to zero (Dirichlet condition).


\section{Geometry}
When we are on a different geometry than $(R,Z)$ the question is how to integrate the field lines. There are two possibilities. 
First, interpolate $R(\zeta_i, \eta_i), Z(\zeta_i, \eta_i)$ for 
all $i$, then integrate in $(R,Z)$ space and finally use
Newton iteration to find $\zeta(R^\pm_i, Z^\pm_i), \eta(R^\pm_i, Z^\pm_i)$. 
The downside here is that it is difficult to tell when and where the fieldline leaves the simulation domain and even worse in MPI the next points might belong to another process. 

The second possibiliy is to integrate entirely in the 
transformed coordinate system $\zeta, \eta$. 
The magnetic field can be easily transformed since we have the
Jacobian of the coordinate transformation
\begin{align}
    B^\zeta(\zeta, \eta) &= \left(\frac{\partial \zeta}{\partial R} B^{R} + \frac{\partial \zeta}{\partial Z}B^Z\right)_{R(\zeta, \eta), Z(\zeta, \eta)} \\
    B^\eta(\zeta, \eta) &= \left(\frac{\partial \eta}{\partial R} B^{R} + \frac{\partial \eta}{\partial Z}B^Z\right)_{R(\zeta, \eta), Z(\zeta, \eta)} \\
    B^\varphi(\zeta, \eta) &= B^\varphi({R(\zeta, \eta), Z(\zeta, \eta)})
    \label{eq:field_trafo}
\end{align}
The advantage is that we can do this for any coordinate
system. Only for cylindrical coordinates, we integrate directly in physical space. The equations to integrate
are
\begin{subequations}
\begin{align}
\frac{\d \zeta}{\d\varphi} &= \frac{B^\zeta}{B^\varphi}\\
\frac{\d \eta}{\d\varphi} &= \frac{B^\eta}{B^\varphi}\\
\frac{\d s}{\d\varphi} &= \frac{|B|}{|B^\varphi|}
\end{align}
\label{eq:fieldlines}
\end{subequations}
The downside here is that when integrating fieldlines we
have to interpolate the magnetic field at arbitrary points. 
However, the error should vanish with $3rd$ order in the 
perpendicular plane. (check this). In order to mitigate this error
we could maybe transform the B-Field on a finer grid/higher order polynomials for more accurate
integration. Also, we need interpolation in the 
first algorithm as well. 

Note that the matrix-matrix multiplications in Eq.~\eqref{eq:sandwich} can
be precomputed and stored. The memory requirements 
in the final computations are 
therefore the same  as in the old version. (Not entirely, since
the diagonal $1/\Delta s$ matrix does not commute with $Q$ or $P$).

Finally remember that the adjoint of a matrix in the modified geometry 
involves the volume element. This means that after you've adjoined the 
parallel derivative the normal way simply bracket the result 
by $1/\sqrt{g}$ and $\sqrt{g}$. 
\section{Algorithm}
Given are the components $v^i(R,Z)$ for $i\in\{R,Z,\varphi\}$ and a compuational grid (in the following the ``coarse grid``)
\begin{itemize}
  \item generate a fine grid by multiplying the cell numbers of the given coarse grid (only topologcially, metric and Jacobian are not needed)
  \item integrate the fieldlines for the fine grid:
    \begin{itemize}
      \item evaluate the starting points on the coarse grid in computational space 
      \item For a curvilinear grid set up a (higher order) grid for the 
        interpolation of the vector components $v^i$ and push forward the vector components
        to the curvilinear coordinate system
      \item Integrate the fieldline equations 
\begin{subequations}
\begin{align}
\frac{\d \zeta}{\d\varphi} &= \frac{v^\zeta}{v^\varphi}\\
\frac{\d \eta}{\d\varphi} &= \frac{v^\eta}{v^\varphi}\\
\frac{\d s}{\d\varphi} &= \frac{1}{|v^\varphi|}
\end{align}
\label{eq:fieldlines_converted}
\end{subequations}
    with the given starting points and $s(0)=0$ from $\varphi=0$ until $\varphi = \pm\Delta \varphi$.
      \item create an interpolation matrix that interpolates from the coarse grid 
        to the fine grid
      \item use the interpolation matrix to generate the plus/minus points for the fine grid
    \end{itemize}
  \item create the interpolation matrices that interpolate from the given coarse grid 
    to the plus/minus points 
  \item create a projection matrix that projects from the fine grid to the coarse grid
  \item compute the matrix-matrix multiplications $P\cdot I^\pm$ as well as the transpose
  \item project the $s$ vectors to the coarse grid
\end{itemize}
\section{MPI implementation}
Let us also note the mpi-implementation, which is not entirely
trivial due to the matrix-matrix multiplications involed in Eq.~\eqref{eq:sandwich}.
\subsection{Row and column distributed sparse matrices}
In Feltor each mpi process gets an equally sized chunk of a 
vector.
Contrary to a vector
a matrix can be distributed in two ways, row-wise and column wise. 
In a row-distributed matrix each process gets the complete 
rows of the matrix that correspond to the indices in the 
vector it holds. 
In a column-distributed matrix each process gets the complete 
columns of the matrix corresponding to the indices in the 
vector it holds. 
When we implement a matrix-vector multiplication the order 
of communication and computation depends on the distribution 
of the matrix.
For the row-distributed matrix each process first has to gather all elements of the input vector it needs to be able to compute the elements of the output. This requires MPI communication.
Formally, the gather operation can be written as a matrix $G$
of $1'$s and $0'$s where the output vector is of equal or larger size than the input vector.
After the elements have been gathered the local matrix-vector
multiplications can be executed.
\begin{align}
M = R\cdot G
\end{align}
where $R$ is the row-distributed matrix with modified indices 
and $G$ is the gather matrix, in which the MPI-communication takes place.

In a column distributed matrix the local matrix-vector multiplication can be executed first because each processor already
has all vector elements it needs. 
However the resuling elements have to be communicated back to 
the process they belong to. Furthermore, a process has to sum
all elements it receives from other processes on the same
index. This is a scatter and reduce operation and
it can be written as a scatter matrix $S$. The transpose
of the scatter matrix is a gather matrix and vice-versa.
\begin{align}
M = S\cdot C
\end{align}
where $S$ is the scatter matrix and $C$ is the column distributed
matrix with modified indices. 

It turns out that a row-distributed matrix can be transposed
by transposition of the local matrices and the gather matrix.
The result is then a column distributed matrix.
The transpose of a column distributed matrix is a row-distributed matrix and vice-versa.

\subsection{Matrix-Matrix multiplication}
We note that we normally construct $\nabla_\parallel^{fc}$ as a column 
distributed
matrix. The advantage is then that the gather operation is bijective, i.e. the transpose of the gather matrix is its inverse. 
This advantage is lost in the present problem. 
It turns out that it is advantageous to construct $\nabla_\parallel^{fc}$
as s row-distributed matrix with global indices. 
This is because a column distributed matrix can be easily (without mpi-communication) multiplied
with a row distributed matrix especially if the indices are global indices. 
Each process just multiplies its local matrices.
\begin{align}
M = C\cdot R
\end{align}
This is not true the other way round. 
The result is then a row distributed matrix with global indices. 
From the global indices the gather map/matrix and the local
indices can be constructed.
We note here that we even don't need to construct the gather matrix
for $\nabla_\parallel^{fc}$, only the one for $\nabla_\parallel^c$ is
needed.



%..................................................................
\bibliography{../references}
%..................................................................


\end{document}

