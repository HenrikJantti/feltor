%\documentclass[12pt]{article}
%\documentclass[12pt]{scrartcl}
\documentclass{hitec} % contained in texlive-latex-extra
\settextfraction{0.9} % indent text
\usepackage{csquotes}
\usepackage[hidelinks]{hyperref} % doi links are short and usefull?
\hypersetup{%
    colorlinks=true,
    linkcolor=blue,
    urlcolor=magenta
}
\urlstyle{rm}
\usepackage[english]{babel}
\usepackage{mathtools} % loads and extends amsmath
\usepackage{amssymb}
% packages not used
%\usepackage{graphicx}
%\usepackage{amsthm}
%\usepackage{subfig}
\usepackage{bm}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{ragged2e} % maybe use \RaggedRight for tables and literature?
\usepackage[table]{xcolor} % for alternating colors
\rowcolors{2}{gray!25}{white}
\renewcommand\arraystretch{1.3}

%%% reset bibliography distances %%%
\let\oldthebibliography\thebibliography
\let\endoldthebibliography\endthebibliography
\renewenvironment{thebibliography}[1]{
  \begin{oldthebibliography}{#1}
    \RaggedRight % remove if justification is desired
    \setlength{\itemsep}{0em}
    \setlength{\parskip}{0em}
}
{
  \end{oldthebibliography}
}
%%% --- %%%

%%%%%%%%%%%%%%%%%%%%%definitions%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\eps}{\varepsilon}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\T}{\mathrm{T}}
\renewcommand{\vec}[1]{{\mathbf{#1}}}
\newcommand{\dx}{\,\mathrm{d}x}
%\newcommand{\dA}{\,\mathrm{d}(x,y)}
%\newcommand{\dV}{\mathrm{d}^3{x}\,}
\newcommand{\dA}{\,\mathrm{dA}}
\newcommand{\dV}{\mathrm{dV}\,}

\newcommand{\Eins}{\mathbf{1}}

\newcommand{\ExB}{$\bm{E}\times\bm{B} \,$}
\newcommand{\GKI}{\int d^6 \bm{Z} \BSP}	
\newcommand{\GKIV}{\int dv_{\|} d \mu d \theta \BSP}	
\newcommand{\BSP}{B_{\|}^*}
\newcommand{\GA}[1]{\langle #1	 \rangle}

\newcommand{\Abar}{\langle A_\parallel \rangle}
%Vectors
\newcommand{\bhat}{\bm{\hat{b}}}
\newcommand{\bbar}{\overline{\bm{b}}}
\newcommand{\chat}{\bm{\hat{c}}}
\newcommand{\ahat}{\bm{\hat{a}}}
\newcommand{\xhat}{\bm{\hat{x}}}
\newcommand{\yhat}{\bm{\hat{y}}}
\newcommand{\zhat}{\bm{\hat{z}}}

\newcommand{\Xbar}{\bar{\vec{X}}}
\newcommand{\phat}{\bm{\hat{\perp}}}
\newcommand{\that}{\bm{\hat{\theta}}}

\newcommand{\eI}{\bm{\hat{e}}_1}
\newcommand{\eII}{\bm{\hat{e}}_2}
\newcommand{\ud}{\mathrm{d}}

%Derivatives etc.
\newcommand{\pfrac}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\ffrac}[2]{\frac{\delta#1}{\delta#2}}
\newcommand{\fixd}[1]{\Big{\arrowvert}_{#1}}
\newcommand{\curl}[1]{\nabla \times #1}
\newcommand{\np}{\nabla_{\perp}}
\newcommand{\npc}{\nabla_{\perp} \cdot }
\newcommand{\nc}{\nabla\cdot }
\newcommand{\GAI}{\Gamma_{1}^{\dagger}}
\newcommand{\GAII}{\Gamma_{1}^{\dagger -1}}
\newcommand{\Tp}{\mathcal T^+_{\Delta\varphi}}
\newcommand{\Tm}{\mathcal T^-_{\Delta\varphi}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%DOCUMENT%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%\preprint{}

\title{The parallel derivative on structured grids}
\author{M.~Wiesenberger}
%\email{Matthias.Wiesenberger@uibk.ac.at}
%\ead{Matthias.Wiesenberger@uibk.ac.at}
%\affiliation{Institute for Ion Physics and Applied Physics, Association EURATOM-\"OAW,  University of
%   Innsbruck, A-6020 Innsbruck, Austria}
%\author{M.~Held}
%\affiliation{Institute for Ion Physics and Applied Physics, Association EURATOM-\"OAW,  University of
   %Innsbruck, A-6020 Innsbruck, Austria}
%\address{Institute for Ion Physics and Applied Physics, Association EURATOM-\"OAW,  Universit\"at 
%   Innsbruck, A-6020 Innsbruck, Austria}
 
\maketitle
\section{The parallel derivative}
The idea of the sandwich method is to bracket the parallel derivative by interpolation and projection matrices:
\begin{align}
    \nabla^c_\parallel &= P\nabla_\parallel^f Q \\
    \nabla^{c\dagger}_\parallel &= P \nabla^{f\dagger}_\parallel Q
    \label{eq:sandwich}
\end{align}
In this way the projection integrals
\begin{align}
    \int\dV \nabla_\parallel f p_i(x)p_j(y) 
    \label{}
\end{align}
are computed more precisely.
The size of the fine grid should therefore be as large as
possible (reasonable? what resolution is needed?).
We first notice that the one interpolation matrix can be absorbed
in the parallel derivative since this also consists of 
interpolation operations. 
\begin{align}
    \nabla^c_\parallel &= P\nabla_\parallel^{fc} \\
    \nabla^{c\dagger}_\parallel &= \nabla^{fc\dagger}_\parallel Q
    \label{eq:sandwich}
\end{align}


In order to understand what the adjoint operators do let us denote $\mathcal T^+_{\Delta\varphi}$ as the push-forwward operator. Then we have
\begin{align}
    \int f(\vec x) \Tp h(\vec x) \sqrt{g(\vec x)}\d^3x \\
    =  \int f(\vec x) h(\Tm \vec x)\sqrt{g(\vec x)}\d^3x \\
    =  \int f(\Tp \vec x') h(\vec x')\sqrt{g(\Tp \vec x')}J^{-1}( \Tp\vec x') \d^3x' \\
    =  \int \frac{1}{\sqrt{g(\vec x')}}\Tm\left[J^{-1}(\vec x')\sqrt{g(\vec x')}f(\vec x')\right] h(\vec x')\sqrt{g(\vec x')}   \d^3x' \\
    \equiv  \int (\Tp)^\dagger\left[f(\vec x)\right] h(\vec x)\sqrt{g(\vec x)}   \d^3x
    \label{}
\end{align}
$J$ is the determinant of the Jacobian $\partial(\vec x')/\partial(\vec x)$.
In the last step we simply replaced the dummy variable $\vec x'$ with $\vec x$ again and identified the relevant terms
as the adjoint operator:
\begin{align}
    (\Tp)^\dagger f(\vec x ) := \frac{1}{\sqrt{g(\vec x)}} \Tm\left[\sqrt{g(\vec x)} J^{-1}(\vec x) f(\vec x) \right]
    \label{}
\end{align}
This means that numerically the adjoint of the push-forward 
operator should be a valid discretization of its inverse. (how can this be exploited as a test?)
Note that $\sqrt{g}J^{-1}(\vec x) = \sqrt{g'(\Tm \vec x)}$.
With this we can write
\begin{align}
    (\Tp)^\dagger f(\vec x ) := \sqrt{\frac{g'(\vec x)}{g(\vec x)}} \Tm\left[f(\vec x) \right]
    \label{}
\end{align}
Also note that while $\Tp [fh] = \Tp f \Tp h$ this might not 
hold on the discrete level. Also the question is how $J$ enters 
on the discrete level. We have to multiply $\sqrt{g}$ artificially when we form the adjoint. 
Theoretically $J$ could be hidden somehow when we integrate the fieldlines, so the information could be contained in the discrete version? (Maybe in the back-projection?)

If the streamlines are divergence free, we have $J=1$.
A numerical test could be ( if we neglect the volume form in the adjoint)
\begin{align}
    (\Tp)^\dagger \left[J(\vec x)\Tp f(\vec x)\right] - f(\vec x) = 0
    \label{}
\end{align}
The numerical computation of $J$ might a bit tricky at the boundaries. 
In a flux-aligned $\zeta, \eta$ it should be feasible but in cylindrical coordinates I don't know how. Maybe we can simply cut the last few cells before the boundary.
Even easier might be
\begin{align}
    \left(\Tp\right)^\dagger J(\vec x ) = 1
    \label{}
\end{align}

If we integrate streamlines of the vector field $\vec B/B^\varphi$, then we have
\begin{align}
    \frac{\d J}{\d \varphi} = J(\vec x ) \nabla\cdot\left( \vec B/B^\varphi\right)
    \label{}
\end{align}
along these streamlines.
Also we have that $\d s = B/B^\varphi \d \varphi $ and the interpolation/projection of $\triangle s$ can probably be neglected. (We want to neglect it because it's memory intensive to store all combinations)
Also this means that when we transpose $\nabla_\parallel$ we get a 
multiplication by $B^\varphi/B$ in the beginning.
In any case this means that we want to have 
\begin{align}
\left(\Tp\right)^\dagger B^\varphi  =  B^\varphi
\end{align}
\section{Boundary conditions}
The question is what to do when a fieldline intersects with the boundary
of the simulation domain before reaching the next plane.
The problem with simply putting a value exactly where the fieldline 
reaches the boundary is first that a true interpolation at that 
position requires a 3d interpolation (we 
are between planes) instead of a 2d interpolation.
Secondly, for Dirichlet boundaries the small 
distance seriously deteriorates the CFL condition.
A simple solution is to set the perpendicular field zero on the 
boundary such that the field becomes purely 
toroidal on the boundary. The fieldlines then have a kink on the 
boundary. On the other hand we can implement boundary conditions consistent with 
the perpendicular ones since the fieldlines never leave the domain. 
We simply interpolate the quantity to derive on the inner side of the
domain boundary (Neumann conditions = "No boundary condition") or 
set the value to zero (Dirichlet condition).


\section{Geometry}
When we are on a different geometry than $(R,Z)$ the question is how to integrate the field lines. There are two possibilities. 
First, interpolate $R(\zeta_i, \eta_i), Z(\zeta_i, \eta_i)$ for 
all $i$, then integrate in $(R,Z)$ space and finally use
Newton iteration to find $\zeta(R^\pm_i, Z^\pm_i), \eta(R^\pm_i, Z^\pm_i)$. 
The downside here is that it is difficult to tell when and where the fieldline leaves the simulation domain and even worse in MPI the next points might belong to another process. 

The second possibiliy is to integrate entirely in the 
transformed coordinate system $\zeta, \eta$. 
The magnetic field can be easily transformed since we have the
Jacobian of the coordinate transformation
\begin{align}
    B^\zeta(\zeta, \eta) &= \left(\frac{\partial \zeta}{\partial R} B^{R} + \frac{\partial \zeta}{\partial Z}B^Z\right)_{R(\zeta, \eta), Z(\zeta, \eta)} \\
    B^\eta(\zeta, \eta) &= \left(\frac{\partial \eta}{\partial R} B^{R} + \frac{\partial \eta}{\partial Z}B^Z\right)_{R(\zeta, \eta), Z(\zeta, \eta)} \\
    B^\varphi(\zeta, \eta) &= B^\varphi({R(\zeta, \eta), Z(\zeta, \eta)})
    \label{eq:field_trafo}
\end{align}
The advantage is that we can do this for any coordinate
system. Only for cylindrical coordinates, we integrate directly in physical space. The equations to integrate
are
\begin{subequations}
\begin{align}
\frac{\d \zeta}{\d\varphi} &= \frac{B^\zeta}{B^\varphi}\\
\frac{\d \eta}{\d\varphi} &= \frac{B^\eta}{B^\varphi}\\
\frac{\d s}{\d\varphi} &= \frac{|B|}{|B^\varphi|}
\end{align}
\label{eq:fieldlines}
\end{subequations}
The downside here is that when integrating fieldlines we
have to interpolate the magnetic field at arbitrary points. 
However, the error should vanish with $3rd$ order in the 
perpendicular plane. (check this). In order to mitigate this error
we could maybe transform the B-Field on a finer grid/higher order polynomials for more accurate
integration. Also, we need interpolation in the 
first algorithm as well. 

Note that the matrix-matrix multiplications in Eq.~\eqref{eq:sandwich} can
be precomputed and stored. The memory requirements 
in the final computations are 
therefore the same  as in the old version. (Not entirely, since
the diagonal $1/\Delta s$ matrix does not commute with $Q$ or $P$).

Finally remember that the adjoint of a matrix in the modified geometry 
involves the volume element. This means that after you've adjoined the 
parallel derivative the normal way simply bracket the result 
by $1/\sqrt{g}$ and $\sqrt{g}$. 
\section{Algorithm}
Given are the components $v^i(R,Z)$ for $i\in\{R,Z,\varphi\}$ and a compuational grid (in the following the ``coarse grid``)
\begin{itemize}
  \item generate a fine grid by multiplying the cell numbers of the given coarse grid (only topologcially, metric and Jacobian are not needed)
  \item integrate the fieldlines for the fine grid:
    \begin{itemize}
      \item evaluate the starting points on the coarse grid in computational space 
      \item For a curvilinear grid set up a (higher order) grid for the 
        interpolation of the vector components $v^i$ and push forward the vector components
        to the curvilinear coordinate system
      \item Integrate the fieldline equations 
\begin{subequations}
\begin{align}
\frac{\d \zeta}{\d\varphi} &= \frac{v^\zeta}{v^\varphi}\\
\frac{\d \eta}{\d\varphi} &= \frac{v^\eta}{v^\varphi}\\
\frac{\d s}{\d\varphi} &= \frac{1}{|v^\varphi|}
\end{align}
\label{eq:fieldlines_converted}
\end{subequations}
    with the given starting points and $s(0)=0$ from $\varphi=0$ until $\varphi = \pm\Delta \varphi$.
      \item create an interpolation matrix that interpolates from the coarse grid 
        to the fine grid
      \item use the interpolation matrix to generate the plus/minus points for the fine grid
    \end{itemize}
  \item create the interpolation matrices that interpolate from the given coarse grid 
    to the plus/minus points 
  \item create a projection matrix that projects from the fine grid to the coarse grid
  \item compute the matrix-matrix multiplications $P\cdot I^\pm$ as well as the transpose
  \item project the $s$ vectors to the coarse grid
\end{itemize}
\section{MPI implementation}
Let us also note the mpi-implementation, which is not entirely
trivial due to the matrix-matrix multiplications involed in Eq.~\eqref{eq:sandwich}.
\subsection{Row and column distributed sparse matrices}
In Feltor each mpi process gets an equally sized chunk of a 
vector.
Contrary to a vector
a matrix can be distributed in two ways, row-wise and column wise. 
In a row-distributed matrix each process gets the complete 
rows of the matrix that correspond to the indices in the 
vector it holds. 
In a column-distributed matrix each process gets the complete 
columns of the matrix corresponding to the indices in the 
vector it holds. 
When we implement a matrix-vector multiplication the order 
of communication and computation depends on the distribution 
of the matrix.
For the row-distributed matrix each process first has to gather all elements of the input vector it needs to be able to compute the elements of the output. This requires MPI communication.
Formally, the gather operation can be written as a matrix $G$
of $1'$s and $0'$s where the output vector is of equal or larger size than the input vector.
After the elements have been gathered the local matrix-vector
multiplications can be executed.
\begin{align}
M = R\cdot G
\end{align}
where $R$ is the row-distributed matrix with modified indices 
and $G$ is the gather matrix, in which the MPI-communication takes place.

In a column distributed matrix the local matrix-vector multiplication can be executed first because each processor already
has all vector elements it needs. 
However the resuling elements have to be communicated back to 
the process they belong to. Furthermore, a process has to sum
all elements it receives from other processes on the same
index. This is a scatter and reduce operation and
it can be written as a scatter matrix $S$. The transpose
of the scatter matrix is a gather matrix and vice-versa.
\begin{align}
M = S\cdot C
\end{align}
where $S$ is the scatter matrix and $C$ is the column distributed
matrix with modified indices. 

It turns out that a row-distributed matrix can be transposed
by transposition of the local matrices and the gather matrix.
The result is then a column distributed matrix.
The transpose of a column distributed matrix is a row-distributed matrix and vice-versa.

\subsection{Matrix-Matrix multiplication}
We note that we normally construct $\nabla_\parallel^{fc}$ as a column 
distributed
matrix. The advantage is then that the gather operation is bijective, i.e. the transpose of the gather matrix is its inverse. 
This advantage is lost in the present problem. 
It turns out that it is advantageous to construct $\nabla_\parallel^{fc}$
as s row-distributed matrix with global indices. 
This is because a column distributed matrix can be easily (without mpi-communication) multiplied
with a row distributed matrix especially if the indices are global indices. 
Each process just multiplies its local matrices.
\begin{align}
M = C\cdot R
\end{align}
This is not true the other way round. 
The result is then a row distributed matrix with global indices. 
From the global indices the gather map/matrix and the local
indices can be constructed.
We note here that we even don't need to construct the gather matrix
for $\nabla_\parallel^{fc}$, only the one for $\nabla_\parallel^c$ is
needed.


\section*{Acknowledgements} 	
This work was supported by the Austrian Science Fund (FWF) W1227-N16 and Y398, and by 
the European Commission under the Contract of Association between EURATOM and \"OAW, carried out within the framework of the European Fusion Development Agreement (EFDA).

%..................................................................
\bibliography{refs}
%\bibliographystyle{aipnum4-1.bst}
\bibliographystyle{model1-num-names}


%..................................................................


\end{document}

